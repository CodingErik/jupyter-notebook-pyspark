{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUFRBP+fWq7JgVGGWBO7zC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodingErik/jupyter-notebook-pyspark/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NSRNsmgblwN",
        "outputId": "43a417fd-c4aa-4aed-83a4-704c80308039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, rand, lit\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"BigDataTest\").getOrCreate()\n",
        "\n",
        "# Generate a large dataset (1M rows)\n",
        "num_rows = 10**6  # 1 million rows\n",
        "data = [(i, f\"User_{i}\") for i in range(num_rows)]\n",
        "\n",
        "# Creating another dataset (Orders)\n",
        "order_data = [(i, f\"Product_{i%10}\", i*10) for i in range(500_000, 1_000_000)]  # Orders from 500K to 1M\n",
        "\n",
        "# creating our dataframes\n",
        "userDF = spark.createDataFrame(data, [\"ID\", \"Name\"])\n",
        "orderDF = spark.createDataFrame(order_data, [\"UserID\", \"Product\", \"Amount\"])\n",
        "\n",
        "# Add a random Age column (0-99) and also adding a Country column with a USA constant\n",
        "userDF = userDF.withColumn(\"Age\", (rand(seed=42) * 100).cast(\"int\"))\n",
        "userDF = userDF.withColumn(\"Country\", lit(\"USA\"))\n",
        "\n",
        "# Show some sample data\n",
        "# df.show(5)\n",
        "\n"
      ],
      "metadata": {
        "id": "jW7dBTIQb28i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "userDF.show(5)\n",
        "\n",
        "userDF.withColumnRenamed()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXOom4Oa3vqi",
        "outputId": "ea00063b-f64f-475c-ba7c-c75f37f9c8fe"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+-------+\n",
            "| ID|  Name|Age|Country|\n",
            "+---+------+---+-------+\n",
            "|  0|User_0| 61|    USA|\n",
            "|  1|User_1| 50|    USA|\n",
            "|  2|User_2| 83|    USA|\n",
            "|  3|User_3| 26|    USA|\n",
            "|  4|User_4| 67|    USA|\n",
            "+---+------+---+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orderDF.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3iwUfH_7Jr6",
        "outputId": "40633ffd-efbe-440b-d034-99a66aaa0eb7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+-------+\n",
            "|UserID|  Product| Amount|\n",
            "+------+---------+-------+\n",
            "|500000|Product_0|5000000|\n",
            "|500001|Product_1|5000010|\n",
            "|500002|Product_2|5000020|\n",
            "|500003|Product_3|5000030|\n",
            "|500004|Product_4|5000040|\n",
            "+------+---------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "inner_join_df = userDF.join(orderDF, userDF.ID == orderDF.UserID, \"inner\")\n",
        "\n",
        "filtered_inner_join_df = inner_join_df.select(userDF.ID, userDF.Name, orderDF.Product, orderDF.Amount).show(5)\n",
        "# inner_join_df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6if7xd3x3cP6",
        "outputId": "d4132a65-805f-45f4-ae84-e420e0b84999"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+---------+-------+\n",
            "|    ID|       Name|  Product| Amount|\n",
            "+------+-----------+---------+-------+\n",
            "|500001|User_500001|Product_1|5000010|\n",
            "|500003|User_500003|Product_3|5000030|\n",
            "|500004|User_500004|Product_4|5000040|\n",
            "|500005|User_500005|Product_5|5000050|\n",
            "|500006|User_500006|Product_6|5000060|\n",
            "+------+-----------+---------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nums = list(range(0, 1000001))\n",
        "len(nums)\n",
        "\n",
        "nums_rdd = spark.sparkContext.parallelize(nums)\n",
        "nums_rdd.limit(10).collect() # it's distributing the list, and this is our driver program"
      ],
      "metadata": {
        "id": "J63tGw1C0IVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, rand, floor, concat, lit, substring\n",
        "\n",
        "# Number of records\n",
        "num_practitioners = 500_000  # Half a million practitioners\n",
        "num_pracRoles = 1_000_000  # One million practitioner roles\n",
        "num_locations = 100_000  # 100K locations\n",
        "\n",
        "# Practitioner DataFrame (500K rows)\n",
        "practitionerDF = spark.range(num_practitioners).toDF(\"PractitionerID\") \\\n",
        "    .withColumn(\"Name\", concat(lit(\"Dr. \"), col(\"PractitionerID\").cast(\"string\"))) \\\n",
        "    .withColumn(\"Specialty\", concat(lit(\"Specialty_\"), (floor(rand(seed=42) * 10)).cast(\"string\")))\n",
        "\n",
        "# Location DataFrame (100K rows)\n",
        "locationDF = spark.range(num_locations).toDF(\"LocationID\") \\\n",
        "    .withColumn(\"FacilityName\", concat(lit(\"Clinic_\"), col(\"LocationID\").cast(\"string\"))) \\\n",
        "    .withColumn(\"City\", concat(lit(\"City_\"), (floor(rand(seed=99) * 50)).cast(\"string\")))\n",
        "\n",
        "# PractitionerRole DataFrame (1M rows) - Driving Table\n",
        "pracRoleDF = spark.range(num_pracRoles).toDF(\"RoleID\") \\\n",
        "    .withColumn(\"PractitionerID\", (floor(rand(seed=21) * num_practitioners)).cast(\"int\")) \\\n",
        "    .withColumn(\"LocationID\", (floor(rand(seed=45) * num_locations)).cast(\"int\")) \\\n",
        "    .withColumn(\"Role\", concat(lit(\"Role_\"), (floor(rand(seed=15) * 5)).cast(\"string\")))\n",
        "\n"
      ],
      "metadata": {
        "id": "V-4C2LCO9mdq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tuning cache behavior\n",
        "spark.conf.set(\"spark.sql.inMemoryColumnarStorage.compressed\", True)  # ✅ Enables compression\n",
        "spark.conf.set(\"spark.sql.inMemoryColumnarStorage.batchSize\", 10000)  # ✅ Adjust batch size for caching"
      ],
      "metadata": {
        "id": "3yz55-2tJY_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.get(\"spark.memory.fraction\")  # Default is 0.6 (60% of JVM heap)\n",
        "spark.conf.get(\"spark.memory.storageFraction\")  # Default is 0.5 (50% of execution memory)"
      ],
      "metadata": {
        "id": "YjN4VT12KqeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check how a df is cached\n",
        "print(pracRoleDF.storageLevel)"
      ],
      "metadata": {
        "id": "QknEzRaOK3bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(spark))  # <class 'pyspark.sql.session.SparkSession'>\n",
        "print(type(pracRoleDF))  # <class 'pyspark.sql.dataframe.DataFrame'>\n",
        "print(type(pracRoleDF.RoleID))  # <class 'pyspark.sql.column.Column'>\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNR3j71IKK5p",
        "outputId": "516f637f-f814-45eb-cc64-4c6579aa8dd9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.session.SparkSession'>\n",
            "<class 'pyspark.sql.dataframe.DataFrame'>\n",
            "<class 'pyspark.sql.column.Column'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "practitionerDF.cache()\n",
        "locationDF.cache()\n",
        "pracRoleDF.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaKqRSpHHIXS",
        "outputId": "c27af81b-ccb5-477c-e6d8-2217be47fcb8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[RoleID: bigint, PractitionerID: int, LocationID: int, Role: string]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "practitionerDF.is_cached\n",
        "locationDF.is_cached\n",
        "pracRoleDF.is_cached"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW71fexbIu1B",
        "outputId": "95b77844-48f4-43e6-d86c-cc46de4b031d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a sample\n",
        "print(\"Practitioner Sample:\")\n",
        "practitionerDF.show(5)\n",
        "\n",
        "print(\"Location Sample:\")\n",
        "locationDF.show(5)\n",
        "\n",
        "print(\"PractitionerRole Sample (Driving Table):\")\n",
        "pracRoleDF.show(5)\n"
      ],
      "metadata": {
        "id": "m62O_gKe9pR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joined_large_df = (\n",
        "    pracRoleDF\n",
        "    .join(practitionerDF, \"PractitionerID\", \"inner\")\n",
        "    .join(locationDF, \"LocationID\", \"left\")\n",
        "    .select(\n",
        "        pracRoleDF.RoleID.alias(\"lito\"),\n",
        "        pracRoleDF.PractitionerID,\n",
        "        practitionerDF.Name,\n",
        "        practitionerDF.Specialty,\n",
        "        locationDF.FacilityName,\n",
        "        locationDF.City\n",
        "    )\n",
        ")\n",
        "# Show a small sample from the massive join\n",
        "joined_large_df.show(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_VouN8Y-IRS",
        "outputId": "bbaaca7a-9980-4466-a7ac-583909f90d06"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------+------+-----------+------------+-------+\n",
            "|RoleID|PractitionerID|  Name|  Specialty|FacilityName|   City|\n",
            "+------+--------------+------+-----------+------------+-------+\n",
            "|160537|             5| Dr. 5|Specialty_5|Clinic_92674|City_31|\n",
            "|414712|             5| Dr. 5|Specialty_5|Clinic_59696|City_36|\n",
            "|470600|             5| Dr. 5|Specialty_5|Clinic_10366| City_2|\n",
            "|620430|             5| Dr. 5|Specialty_5|Clinic_15243|City_14|\n",
            "|970123|             5| Dr. 5|Specialty_5|Clinic_29953| City_3|\n",
            "|972503|             5| Dr. 5|Specialty_5|Clinic_10502|City_42|\n",
            "|363365|             7| Dr. 7|Specialty_0|Clinic_14545|City_21|\n",
            "|183093|             9| Dr. 9|Specialty_7|Clinic_69161|City_37|\n",
            "| 39954|            10|Dr. 10|Specialty_4|Clinic_45239|City_25|\n",
            "|344445|            10|Dr. 10|Specialty_4|Clinic_21695| City_5|\n",
            "+------+--------------+------+-----------+------------+-------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}